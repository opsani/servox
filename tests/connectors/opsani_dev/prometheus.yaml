---
apiVersion: v1
kind: ConfigMap
metadata:
  name: servo-config
data:
  optimizer: fiber-http-optimizer
  log_level: DEBUG
  servo.yaml: |
    kubernetes:
      settlement: 5m
      deployments:
        - name: fiber-http
          strategy:
            type: canary
            alias: tuning
          replicas:
            min: 1
            max: 4
            step: 1
          containers:
          - name: fiber-http
            alias: main
            cpu:
              min: 500m
              max: 2.0
              step: 0.125
            memory:
              min: 128 MiB
              max: 3 GiB
              step: 0.125

    prometheus:
      # NOTE: In a sidecar configuration, Prometheus is colocated with the servo in the Pod
      base_url: http://localhost:9090
      metrics:
        - name: main_instance_count
          query: sum(envoy_cluster_membership_healthy{opsani_role!="tuning"}) OR ON() vector(0)
          unit: count
        - name: tuning_instance_count
          query: envoy_cluster_membership_healthy{opsani_role="tuning"} OR ON() vector(0)
          unit: count

        - name: main_pod_avg_request_rate
          query: avg(rate(envoy_cluster_upstream_rq_total{opsani_role!="tuning"}[3m])) OR ON() vector(0)
          unit: rps
        - name: total_request_rate
          query: sum(rate(envoy_cluster_upstream_rq_total[3m])) OR ON() vector(0)
          unit: rps
        - name: main_request_rate
          query: sum(rate(envoy_cluster_upstream_rq_total{opsani_role!="tuning"}[3m])) OR ON() vector(0)
          unit: rps
        - name: tuning_request_rate
          query: rate(envoy_cluster_upstream_rq_total{opsani_role="tuning"}[3m]) OR ON() vector(0)
          unit: rps

        - name: main_success_rate
          query: sum(rate(envoy_cluster_upstream_rq_xx{opsani_role!="tuning", envoy_response_code_class="2"}[3m])) OR ON() vector(0)
          unit: rps
        - name: tuning_success_rate
          query: rate(envoy_cluster_upstream_rq_xx{opsani_role="tuning", envoy_response_code_class="2"}[3m]) OR ON() vector(0)
          unit: rps

        - name: main_error_rate
          query: sum(rate(envoy_cluster_upstream_rq_xx{opsani_role!="tuning", envoy_response_code_class=~"4|5"}[3m])) OR ON() vector(0)
          unit: rps
        - name: tuning_error_rate
          query: rate(envoy_cluster_upstream_rq_xx{opsani_role="tuning", envoy_response_code_class=~"4|5"}[3m]) OR ON() vector(0)
          unit: rps

        - name: main_p99_latency
          query: avg(histogram_quantile(0.99,rate(envoy_cluster_upstream_rq_time_bucket{opsani_role!="tuning"}[3m]))) OR ON() vector(0)
          unit: ms
        - name: tuning_p99_latency
          query: avg(histogram_quantile(0.99,rate(envoy_cluster_upstream_rq_time_bucket{opsani_role="tuning"}[3m]))) OR ON() vector(0)
          unit: ms
        - name: main_p90_latency
          query: avg(histogram_quantile(0.9,rate(envoy_cluster_upstream_rq_time_bucket{opsani_role!="tuning"}[3m]))) OR ON() vector(0)
          unit: ms
        - name: tuning_p90_latency
          query: avg(histogram_quantile(0.9,rate(envoy_cluster_upstream_rq_time_bucket{opsani_role="tuning"}[3m]))) OR ON() vector(0)
          unit: ms
        - name: main_p50_latency
          query: avg(histogram_quantile(0.5,rate(envoy_cluster_upstream_rq_time_bucket{opsani_role!="tuning"}[3m]))) OR ON() vector(0)
          unit: ms
        - name: tuning_p50_latency
          query: avg(histogram_quantile(0.5,rate(envoy_cluster_upstream_rq_time_bucket{opsani_role="tuning"}[3m]))) OR ON() vector(0)
          unit: ms
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: servo
  labels:
    app.kubernetes.io/name: servo
    app.kubernetes.io/component: core
spec:
  replicas: 1
  revisionHistoryLimit: 2
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: servo
  template:
    metadata:
      name: servo
      labels:
        app.kubernetes.io/name: servo
        app.kubernetes.io/component: core
    spec:
      # serviceAccountName: servo
      containers:
      - name: prometheus
        image: quay.io/prometheus/prometheus:v2.20.1
        args:
          - '--storage.tsdb.retention.time=12h'
          - '--config.file=/etc/prometheus/prometheus.yaml'
        ports:
        - name: webui
          containerPort: 9090
        resources:
          requests:
            cpu: 100m
            memory: 128M
          limits:
            cpu: 500m
            memory: 1G
        volumeMounts:
        - name: prometheus-config-volume
          mountPath: /etc/prometheus
      volumes:
      - name: servo-token-volume
        secret:
          secretName: servo-token
          items:
          - key: token
            path: opsani.token
      - name: servo-config-volume
        configMap:
          name: servo-config
          items:
          - key: servo.yaml
            path: servo.yaml
      - name: prometheus-config-volume
        configMap:
          name: prometheus-config

      # Prefer deployment onto a Node labeled role=servo
      # This ensures physical isolation and network transport if possible
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: node.opsani.com/role
                operator: In
                values:
                - servo
---
# apiVersion: v1
# kind: Secret
# metadata:
#   name: servo-token
#   labels:
#     app.kubernetes.io/name: servo
#     app.kubernetes.io/component: core
# type: Opaque
# data:
#   token: $OPSANI_TOKEN_BASE64

# ---
# apiVersion: v1
# kind: ServiceAccount
# metadata:
#   name: servo
#   labels:
#     app.kubernetes.io/name: servo
#     app.kubernetes.io/component: core

# ---
# Cluster Role for the servo itself
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRole
# metadata:
#   name: servo
#   labels:
#     app.kubernetes.io/name: servo
#     app.kubernetes.io/component: core
# rules:
# - apiGroups: ["apps", "extensions"]
#   resources: ["deployments", "replicasets"]
#   verbs: ["get", "list", "watch", "update", "patch"]
# - apiGroups: [""]
#   resources: ["pods", "pods/logs", "pods/status"]
#   verbs: ["create", "delete", "get", "list", "watch" ]
# - apiGroups: [""]
#   resources: ["namespaces"]
#   verbs: ["get", "list"]

# ---
# # Cluster Role for the Prometheus sidecar
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRole
# metadata:
#   name: prometheus
#   labels:
#     app.kubernetes.io/name: prometheus
#     app.kubernetes.io/component: metrics
#     app.kubernetes.io/part-of: servo
# rules:
# - apiGroups: [""]
#   resources:
#   - nodes
#   - nodes/proxy
#   - services
#   - endpoints
#   - pods
#   verbs: ["get", "list", "watch"]
# - apiGroups: [""]
#   resources:
#   - configmaps
#   verbs: ["get"]
# - nonResourceURLs: ["/metrics"]
#   verbs: ["get"]

# ---
# # Bind the Servo Cluster Role to the servo Service Account
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRoleBinding
# metadata:
#   name: servo
#   labels:
#     app.kubernetes.io/name: servo
#     app.kubernetes.io/component: core
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: ClusterRole
#   name: servo
# subjects:
# - kind: ServiceAccount
#   name: servo

# ---
# # Bind the Prometheus Cluster Role to the servo Service Account
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRoleBinding
# metadata:
#   name: prometheus
#   labels:
#     app.kubernetes.io/name: prometheus
#     app.kubernetes.io/component: metrics
#     app.kubernetes.io/part-of: servo
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: ClusterRole
#   name: prometheus
# subjects:
# - kind: ServiceAccount
#   name: servo

# ---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: servo
data:
  prometheus.yaml: |
    # Opsani Servo Prometheus Sidecar v0.6.2
    # This configuration allows the Opsani Servo to discover and scrape Pods that
    # have been injected with an Envoy proxy sidecar container that emits the metrics
    # necessary for optimization. Scraping by the Prometheus sidecar is enabled by
    # adding the following annotations to the Pod spec of the Deployment under
    # optimization:
    #
    # annotations:
    #   prometheus.opsani.com/path: /stats/prometheus # Default Envoy metrics path
    #   prometheus.opsani.com/port: "9901" # Default Envoy metrics port
    #   prometheus.opsani.com/scrape: "true" # Opt-in for scraping by the servo
    #
    # Path and port collisions with the optimization target can be resolved be changing
    # the relevant annotation.

    # Scrape the targets every 5 seconds.
    # Since we are only looking at specifically annotated Envoy sidecar containers
    # with a known metrics surface area and retain the values for <= 24 hours, we
    # can scrape aggressively. The higher scrape resolution is helpful for testing
    # and running checks that verify configuration health.
    global:
      scrape_interval: 5s
      scrape_timeout: 5s
      evaluation_interval: 5s

    # Scrape the Envoy sidecar metrics based on matching annotations (see above)
    scrape_configs:
    - job_name: 'opsani-envoy-sidecars'
      kubernetes_sd_configs:
        - role: pod
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_opsani_com_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_opsani_com_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_opsani_com_port]
          action: replace
          target_label: __address__
          regex: (.+)(?::\d+);(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
