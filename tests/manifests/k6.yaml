---
apiVersion: apps/v1
kind: Deployment

metadata:
  name: k6
  labels:
    app.kubernetes.io/name: k6

spec:
  replicas: 1
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: k6
  template:
    metadata:
      labels:
        app.kubernetes.io/name: k6
    spec:
      # Prefer deployment onto a Node labeled node.opsani.com=servo
      # This ensures physical isolation and network transport if possible
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: node.opsani.com/role
                operator: In
                values:
                - servo
      containers:
      - name: k6
        image: loadimpact/k6:latest
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        command: ["k6"]
        args: ["run", "/scripts/fiber-http-varload.js"]
        resources:
          requests:
            cpu: "1"
            memory: 1G
          limits:
            cpu: "4"
            memory: 8G
        volumeMounts:
        - name: k6-scripts-volume
          mountPath: /scripts
      volumes:
      - name: k6-scripts-volume
        configMap:
          name: k6-scripts
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: k6-scripts
data:
  fiber-http-varload.js: |
    import http from 'k6/http';

    export let options = {
      // Don't bother with reading the response as we don't care
      discardResponseBodies: true,

      httpDebug: 'full',

      /*
      IMPORTANT - Managing Connection Reuse

      In order to get traffic routed to a Pod after an adjust operation,
      we must disable TCP connection reuse across iterations. By default, k6
      pools connections between VUs to avoid TCP connection setup and SSL
      handshake overhead. When a Pod is restarted following an adjust, its
      connections are evicted from the pool and when it rejoins the Service
      Load Balancer k6 will not send it any traffic because it will reuse the
      remaining connection pool for the duration of the test.

      The `noVUConnectionReuse` option causes k6 to establish a new TCP connection
      between every *iteration* (effectively one trip through the `default`
      function below). Disable the reuse behavior does come at significant cost:
      TCP and SSL overhead quickly become the limiting factor for the request
      rate you can obtain. This impact can be mitigated by batching requests in
      the default function. During the iteration, each VU will reuse a TCP connection
      and leverage HTTP keep-alives on the requests.

      Find a target duration and/or number of requests to issue within each iteration
      in order to achieve the desired traffic targets.
      */
      noVUConnectionReuse: true,

      // Specifies the minimum runtime duration for each iteration (or exection of the
      // default function. k6 will put the VU to sleep if it runs out of work before the
      // target duration has expired. Default value of 0 disables enforcement.
      minIterationDuration: 0,

      scenarios: {
        requests: {
          // Execute a variable rate test with iteration targets
          executor: 'ramping-arrival-rate',

          //startTime: '5m',

          // Start at zero iterations and then ramp up to stage 1
          startRate: 50,

          // Anchor iterations to 10 second durations (see default function below)
          timeUnit: '10s',

          // Allocate enough VUs to support the entire test or the executor throttles
          preAllocatedVUs: 10,

          /**
          STAGES

            The stages define our variable rate targets and their duration.
            Note that these targets are in terms of *iterations* per time unit --
            they are not requests per second. RPS must be calculated by considering
            the number of requests that you are issuing in the default function
            over the duration of each iteration. The load will be evenly balanced
            but you can't stay anchored in constant rate RPS thinking.
          */
        stages: buildStepPyramidStages({
          baseRate: 50,
          targetRate: 450,
          stepRate: 50,
          rampDuration: '10m',
          soakDuration: '5m'
        }),

        // NOTE: This will accelerate across the duration perfectly
        // stages: buildCrescendoStages({
        //   baseRate: 0,
        //   targetRate: 450,
        //   stepRate: 50,
        //   rampDuration: '10m'
        //  }),
        },
      },
    };

    export default function () {
      /**
      * In order to retain the persistent connection, enter a timed loop
      * and issue requests as fast as possible until the duration expires.
      * The next time that the VU runs it will establish a new connection
      * to the load balancer and reconnect to an arbitrary backend node.
      */
      const durationInSeconds = 10;
      const url = `http://fiber-http.${__ENV.POD_NAMESPACE}.svc.cluster.local/`;
      const requests = buildBatch(url, 10);

      console.log("LOADING URL: ", url)

      const before = new Date().getTime();
      while (true) {
        // issue a single synchronous request
        // http.get(url);

        // alternately, issue a batch of parallel requests
        http.batch(requests);

        // break out once the duration expires
        const after = new Date().getTime();
        const diff = (after - before) / 1000;
        const remainder = durationInSeconds - diff;
        if (remainder <= 0) {
          break;
        }
      }
    };

    function buildCrescendoStages(options) {
      let ascent = [];
      let rate = options.baseRate.valueOf();
      while (rate < options.targetRate) {
        ascent.push({
          target: rate,
          duration: options.rampDuration
        });
        if ("soakDuration" in options) {
          ascent.push({
            target: rate,
            duration: options.soakDuration,
          });
        }

        rate = rate + options.stepRate;
      }

      // Add the peakrate
      ascent.push({
        target: options.targetRate,
        duration: options.rampDuration
      });
      if ("soakDuration" in options) {
        ascent.push({
          target: options.targetRate,
          duration: options.soakDuration,
        });
      }

      return ascent;
    }

    /**
    * Generate a variable rate load pyramid that ascends and descends in rate across stages.
    *
    * @param {object} options Options for the load pyramid.
    * @param {int} options.baseRate The rate for the initial stage (delta from `startRate` option).
    * @param {int} options.peakRate The maximum rate to step the load up to.
    * @param {int} options.stepRate The rate increase or decrease for each step.
    * @param {string} options.rampDuration The time period to ramp the rate change over (Golang duration string).
    * @param {string} options.soakDuration The time period to apply steady state load at following a change.
    */
    function buildStepPyramidStages(options) {
      let ascent = buildCrescendoStages(options);
      const descent = Array.from(ascent).reverse();
      return ascent.concat(descent);
    }

    // TODO: build bipolar stages: zig zag from high to low. 5+ mins transition to allow HPA to move

    /**
    * Return a batch for loading one URL multiple times in parallel.
    *
    * @param {string} url The URL to load.
    * @param {int} size The number of times to load it in parallel.
    */
    function buildBatch(url, size) {
      let request = {
        method: 'GET',
        url: url,
      };
      let requests = [];
      for (let i = 0; i < size; i++) {
        requests.push(request);
      }
      return requests;
    }
